{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnKu+kS3ejTU9jcMZMPuJY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaseebAhmed-official/PROJECTS_Qaurter_02/blob/main/Project_01(Langchain%22HELLO_WORLD%22).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üöÄ 1. Installing Required Libraries**"
      ],
      "metadata": {
        "id": "RmvG1H8SNyDV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDvoYZ-ozZMo"
      },
      "outputs": [],
      "source": [
        "# üöÄ Importing necessary libraries\n",
        "# Install the required library for integrating with Google Generative AI\n",
        "!pip install langchain_google_genai --upgrade --q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üîç Explanation:**\n",
        "- **`!pip install`:** Installs the required Python package.\n",
        "- **`langchain_google_genai`:** The package to interact with Google Generative AI.\n",
        "- **`--upgrade`:** Ensures you get the latest version of the package.\n",
        "- **`--q`:** Suppresses unnecessary logs during installation."
      ],
      "metadata": {
        "id": "jMDJLXZjOAs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üìö 2. Importing Necessary Libraries**"
      ],
      "metadata": {
        "id": "H6TYRtliH-3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the GoogleGenerativeAI class from the LangChain library\n",
        "from langchain_google_genai import GoogleGenerativeAI"
      ],
      "metadata": {
        "id": "RR99SmPc0htl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîç Explanation:\n",
        "- **`from langchain_google_genai import GoogleGenerativeAI`:**\n",
        "  - Imports the class to interact with Google's Generative AI models.\n",
        "- **`from google.colab import userdata`:**\n",
        "  - Provides secure access to user-specific data in Google Colab."
      ],
      "metadata": {
        "id": "sIUV3rbtOQkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üîë 3. Accessing User-Specific API Key Securely**"
      ],
      "metadata": {
        "id": "MxD7wd_0IPPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîë Accessing user-specific API key securely\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY') # Fetch the API key stored securely\n"
      ],
      "metadata": {
        "id": "NtkYllzH0t79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üîç Explanation:**\n",
        "- **`userdata.get('GOOGLE_API_KEY')`:**\n",
        "  - Retrieves the API key stored securely in Google Colab.\n",
        "- **Why Secure Storage?**\n",
        "  - Protects sensitive information from being exposed or hardcoded."
      ],
      "metadata": {
        "id": "VnIGOu79OcUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. **Setting Up Questions üìù**\n",
        "In this section, we're **creating a list of questions** that the AI model will process and provide answers to. Each question is carefully formulated to extract meaningful and detailed responses."
      ],
      "metadata": {
        "id": "LGsBRQ-YIfEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìù List of questions to ask the AI\n",
        "questions = [\n",
        "    \"What is a web API, and how does it work?\",\n",
        "    \"Can you explain LANGCHAIN?\",\n",
        "    \"What is the capital city of France?\"\n",
        "]"
      ],
      "metadata": {
        "id": "jPzZWR4K02UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üîç Explanation:**\n",
        "- **`questions`:**\n",
        "  - A list containing the queries to be processed by the AI.\n",
        "- **Purpose of Each Question:**\n",
        "  - **Q1:** Understand Web APIs.\n",
        "  - **Q2:** Learn about LangChain.\n",
        "  - **Q3:** Test general knowledge."
      ],
      "metadata": {
        "id": "NILAxXSJOpQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üß† 5. Setting Up the Prompt Template**"
      ],
      "metadata": {
        "id": "lFyqXtpLPA7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üß† Setting up the prompt template\n",
        "from langchain.prompts import PromptTemplate\n",
        "# A prompt template formats questions in a specific way before sending them to the AI\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],   # Placeholders for inputs\n",
        "    template=\"I need assistance. What can you tell me about: {question}\" # Template structure\n",
        ")"
      ],
      "metadata": {
        "id": "2In9PNQr75MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üîç Explanation:**\n",
        "- **`PromptTemplate`:**\n",
        "  - Creates a structured format for the questions.\n",
        "- **Components:**\n",
        "  - **`input_variables`:** Placeholders for dynamic inputs.\n",
        "  - **`template`:** The fixed format of the prompt."
      ],
      "metadata": {
        "id": "-56M-LZAPHHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ü§ñ 6. Configuring the AI Model**"
      ],
      "metadata": {
        "id": "Yczh2JMWPSVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üåü Configuring the Language Learning Model (LLM)\n",
        "llm = GoogleGenerativeAI(\n",
        "    api_key=GOOGLE_API_KEY, # Your API key for accessing Google's Generative AI\n",
        "    model=\"gemini-2.0-flash-exp\",  # Model version\n",
        "    temperature=0.5, # Adjust creativity (0.5 = balanced creativity)\n",
        "    max_tokens=200  # Limit the length of responses\n",
        "    )"
      ],
      "metadata": {
        "id": "_WrusuND8fZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üîç Explanation:**\n",
        "- **`api_key`:** Used to authenticate API requests.\n",
        "- **`model`:** Specifies the AI model (e.g., `gemini-2.0-flash-exp`).\n",
        "- **`temperature`:** Controls creativity (0.5 = balanced).\n",
        "- **`max_tokens`:** Limits the response length to 200 tokens."
      ],
      "metadata": {
        "id": "9BRaePPcPUVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üîó 7. Connecting Template and Model (Chaining)**"
      ],
      "metadata": {
        "id": "IFxgOc-uPas7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîó Chaining the template and the LLM\n",
        "# The prompt template and LLM are connected to create a pipeline\n",
        "Chain = prompt_template | llm"
      ],
      "metadata": {
        "id": "SwIPmN728Xon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üîç Explanation:**\n",
        "- **`|`:**\n",
        "  - Chains the `PromptTemplate` and the `GoogleGenerativeAI` model.\n",
        "  - Automatically processes questions through the pipeline."
      ],
      "metadata": {
        "id": "84Mzm_HrPiZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üí¨ 8. Generating Responses**"
      ],
      "metadata": {
        "id": "6SxlGBhhPoZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üì¶ Collecting responses\n",
        "responses = [] # Initialize an empty list to store responses\n",
        "for question in questions: # Loop through each question\n",
        "    response = Chain.invoke({\"question\": question}) # Generate a response for the current question\n",
        "    responses.append(response) # Add the response to the list"
      ],
      "metadata": {
        "id": "9COV7WGR8jtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üîç Explanation:**\n",
        "- **`responses`:** Initializes an empty list to store the AI responses.\n",
        "- **Loop:**\n",
        "  - Iterates through each question in the `questions` list.\n",
        "  - Sends the question to the AI model using `Chain.invoke`.\n",
        "  - Appends the response to the `responses` list."
      ],
      "metadata": {
        "id": "ZzkUHzFSPuKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üé® 9. Displaying Responses in Markdown Format**"
      ],
      "metadata": {
        "id": "WT2olBncPywR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üé® Displaying responses in Markdown format\n",
        "from IPython.display import display, Markdown\n",
        "for i, response in enumerate(responses):# Loop through each response with its index\n",
        "    display(Markdown(f\"**Question {i+1}: {questions[i]}**\\n\\n{response}\")) # Display question and response in Markdown\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "collapsed": true,
        "id": "F54AztAJ912D",
        "outputId": "4a5cddd3-7e72-40a8-c625-10245f1de1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Question 1: What is a web API, and how does it work?**\n\nOkay, let's break down what a Web API is and how it works.\n\n**What is a Web API?**\n\nAt its core, a Web API (Application Programming Interface) is a way for different software applications to communicate with each other over the internet. Think of it as a messenger or a translator that allows applications to exchange data and functionality.\n\nHere's a more detailed explanation:\n\n* **API:** An API defines a set of rules and specifications that dictate how software components should interact. It outlines the methods, data formats, and protocols that can be used to request and receive information or trigger actions.\n* **Web API:** A Web API specifically uses web technologies (like HTTP) to enable communication between applications over the internet. It's essentially an API that's accessible through web addresses (URLs).\n\n**Think of it like ordering food at a restaurant:**\n\n* **You (the application) are the client:** You want to get some information or"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Question 2: Can you explain LANGCHAIN?**\n\nOkay, let's dive into LangChain! I can definitely help you understand what it is and what it does.\n\n**LangChain: The Basics**\n\nAt its core, LangChain is a **framework for developing applications powered by large language models (LLMs)**. Think of it as a toolbox or a set of building blocks that makes it easier for developers to create sophisticated AI applications that go beyond simple text generation.\n\nHere's a breakdown of key concepts:\n\n* **LLM Interaction:** LangChain provides a standardized interface for working with various LLMs, like OpenAI's GPT models, Google's PaLM, and many others. This means you don't have to write different code for each model; LangChain abstracts away those differences.\n* **Chaining and Composition:** The \"Chain\" part of the name is crucial. LangChain allows you to combine multiple actions or components into a sequence, creating more complex workflows. For example, you might chain together"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Question 3: What is the capital city of France?**\n\nOkay, I can definitely help you with that!\n\nThe capital city of France is **Paris**.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **üîç Explanation:**\n",
        "- **`IPython.display`:**\n",
        "  - Used to render outputs in rich formats like Markdown.\n",
        "- **`Markdown`:**\n",
        "  - Formats the responses for clean and readable display.\n",
        "- **Loop:**\n",
        "  - Iterates through the `responses` list.\n",
        "  - Displays each question and its corresponding response in Markdown format."
      ],
      "metadata": {
        "id": "KNjWVXkEP4p8"
      }
    }
  ]
}