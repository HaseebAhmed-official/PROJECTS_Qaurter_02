{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTParqETf1DZHatgXhP/I/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaseebAhmed-official/PROJECTS_Qaurter_02/blob/main/Project_LangChain_RAG_with_Google_Gemini_Flash_and_Pincone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üéâ Install the LangChain Google Generative AI library\n",
        "# The -q flag makes the installation quiet, and -U ensures the library is upgraded to the latest version.\n",
        "!pip install -qU langchain-google-genai\n",
        "\n",
        "# üîë Import the userdata module from Google Colab\n",
        "# This module is used to securely retrieve sensitive data, like API keys.\n",
        "from google.colab import userdata\n",
        "\n",
        "# ü§î Retrieve the Google API key\n",
        "# The 'userdata.get' function fetches the API key you securely stored in Colab.\n",
        "userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# üåé Import the 'os' module\n",
        "# This is a standard Python module that allows interaction with the operating system, such as setting environment variables.\n",
        "import os\n",
        "\n",
        "# üîí Set the API key as an environment variable\n",
        "# Environment variables securely store sensitive information like API keys.\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n"
      ],
      "metadata": {
        "id": "PfK5O92T0i3L"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. `!pip install -qU langchain-google-genai`**\n",
        "- **`!`**: In Jupyter or Colab, the `!` symbol allows running shell commands directly from the notebook.\n",
        "- **`pip`**: The Python package manager, used to install libraries and tools.\n",
        "- **`install`**: The command that tells `pip` to download and install a library.\n",
        "- **`-q`**: Stands for \"quiet.\" This reduces the amount of text output during installation.\n",
        "- **`-U`**: Stands for \"upgrade.\" Ensures the library is updated to the latest version.\n",
        "- **`langchain-google-genai`**: The library for integrating LangChain with Google Generative AI models like Gemini.\n",
        "\n",
        "üõ†Ô∏è **This installs a powerful tool for AI workflows!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. `from google.colab import userdata`**\n",
        "- **`from`**: A Python keyword used to import specific modules or parts of a library.\n",
        "- **`google.colab`**: A module available in Google Colab for interacting with the notebook environment.\n",
        "- **`import`**: This tells Python to load a specific module or tool.\n",
        "- **`userdata`**: A submodule in Google Colab that helps manage sensitive user data securely, like API keys.\n",
        "\n",
        "üß© **This imports the key-retrieval tool!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. `userdata.get('GOOGLE_API_KEY')`**\n",
        "- **`userdata`**: Refers to the module imported earlier, used for secure storage and retrieval of sensitive data.\n",
        "- **`get`**: A method that fetches a stored value.\n",
        "- **`'GOOGLE_API_KEY'`**: A key name representing your Google API key stored in Colab.\n",
        "\n",
        "üîë **Fetching the key to unlock Google services!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. `import os`**\n",
        "- **`import`**: Loads a library into Python for use in the script.\n",
        "- **`os`**: A built-in Python module for interacting with the operating system. It allows setting and reading environment variables, working with files, and more.\n",
        "\n",
        "üåç **Bringing the operating system into the code!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. `os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')`**\n",
        "- **`os.environ`**: A dictionary-like object in the `os` module that stores and retrieves environment variables.\n",
        "- **`['GOOGLE_API_KEY']`**: A key in the `os.environ` dictionary. This key holds the value of your Google API key.\n",
        "- **`=`**: Assignment operator. It assigns the value on the right to the variable on the left.\n",
        "- **`userdata.get('GOOGLE_API_KEY')`**: Fetches the Google API key securely and assigns it to the `GOOGLE_API_KEY` environment variable.\n",
        "---\n",
        "\n",
        "\n",
        "|**Code**                          | **Explanation**                                                                                 | **Highlights**                      |\n",
        "|-------------------------------------------|-------------------------------------------------------------------------------------------------|----------------------------------|\n",
        "| `!pip install -qU langchain-google-genai` | Installs the LangChain integration for Google Generative AI with quiet output and latest version. | üõ†Ô∏è Installing AI tools!          |\n",
        "| `from google.colab import userdata`       | Imports the `userdata` module to securely manage sensitive data in Google Colab.                | üîë Secure key management!         |\n",
        "| `userdata.get('GOOGLE_API_KEY')`          | Retrieves the Google API key stored securely in Colab.                                          | üîì Unlocking the API key!         |\n",
        "| `import os`                               | Loads the `os` module, allowing interaction with the operating system.                          | üåç OS helper!                    |\n",
        "| `os.environ['GOOGLE_API_KEY'] = ...`      | Stores the Google API key as an environment variable for secure access by the code.             | üîí Securely setting API key!      |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CEra2LLFTBb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üéâ Install the LangChain Pinecone integration library\n",
        "# -q: Quiet installation, -U: Upgrade to the latest version.\n",
        "!pip install -qU langchain-pinecone\n",
        "\n",
        "# üìö Import the Pinecone library\n",
        "# Pinecone is a vector database for storing and retrieving embeddings.\n",
        "from pinecone import Pinecone\n",
        "\n",
        "# üîë Import the userdata module from Google Colab\n",
        "# Used to securely manage sensitive information, like API keys.\n",
        "from google.colab import userdata\n",
        "\n",
        "# üåê Set the Pinecone API key as an environment variable\n",
        "# Retrieves the API key from Colab's secure storage and sets it in the environment.\n",
        "PINECONE_API_KEY=os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "# üöÄ Initialize the Pinecone client\n",
        "# Creates a Pinecone object using the API key stored in the environment variable.\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n"
      ],
      "metadata": {
        "id": "mhc1RmN90pGU"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. `!pip install -qU langchain-pinecone`**\n",
        "- **`!`**: Allows you to run shell commands (like `pip install`) directly in Jupyter or Colab.\n",
        "- **`pip`**: Python‚Äôs package manager, used to install libraries.\n",
        "- **`install`**: Command to install a specific library.\n",
        "- **`-q`**: Stands for \"quiet\" mode, reducing the output during installation.\n",
        "- **`-U`**: Ensures the library is upgraded to its latest version.\n",
        "- **`langchain-pinecone`**: Library that integrates LangChain with Pinecone for managing vector data.\n",
        "\n",
        "üì¶ **Installing tools for vector databases!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. `from pinecone import Pinecone`**\n",
        "- **`from`**: Python keyword to import specific parts of a library.\n",
        "- **`pinecone`**: The Python client library for Pinecone.\n",
        "- **`import`**: Loads the specified module or class.\n",
        "- **`Pinecone`**: A class that represents the Pinecone client, allowing you to connect to and interact with the Pinecone vector database.\n",
        "\n",
        "üß© **Loading the Pinecone client!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. `from google.colab import userdata`**\n",
        "- **`from`**: Specifies that we‚Äôre importing from the `google.colab` module.\n",
        "- **`google.colab`**: Module specific to Google Colab for managing notebook operations and user data.\n",
        "- **`import`**: Loads the `userdata` submodule.\n",
        "- **`userdata`**: A tool for securely managing sensitive user data like API keys.\n",
        "\n",
        "üîê **Ensuring secure API key management!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. `os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')`**\n",
        "- **`os.environ`**: A dictionary-like object in the `os` module that manages environment variables.\n",
        "- **`['PINECONE_API_KEY']`**: Defines a new environment variable named `PINECONE_API_KEY`.\n",
        "- **`=`**: Assigns a value to the variable.\n",
        "- **`userdata.get('PINECONE_API_KEY')`**:\n",
        "  - Fetches the API key securely stored in Colab's `userdata`.\n",
        "\n",
        "üîë **Setting Pinecone‚Äôs API key securely!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. `pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))`**\n",
        "- **`pc`**: A variable holding the initialized Pinecone client object.\n",
        "- **`Pinecone()`**: Instantiates the Pinecone client.\n",
        "- **`api_key`**: Parameter that authenticates the connection to your Pinecone account.\n",
        "- **`os.environ.get(\"PINECONE_API_KEY\")`**:\n",
        "  - Retrieves the value of the `PINECONE_API_KEY` environment variable.\n",
        "\n",
        "üöÄ **Connecting to the Pinecone database!**\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Tabular Breakdown**\n",
        "\n",
        "|   **Code**                                | **Explanation**                                                                 | **Highlights**                      |\n",
        "|---------------------------------------------------|---------------------------------------------------------------------------------|----------------------------------|\n",
        "| `!pip install -qU langchain-pinecone`            | Installs the LangChain-Pinecone integration library.                            | üì¶ Installing Pinecone tools!    |\n",
        "| `from pinecone import Pinecone`                  | Imports the Pinecone client to interact with the vector database.               | üß© Loading the Pinecone client!  |\n",
        "| `from google.colab import userdata`              | Imports Colab‚Äôs `userdata` module for secure API key management.                | üîê Secure API key management!    |\n",
        "| `os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')` | Sets the Pinecone API key as an environment variable.                           | üîë Setting the API key securely! |\n",
        "| `pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))` | Initializes a Pinecone client using the API key stored in the environment.       | üöÄ Connecting to Pinecone!       |\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "This code:\n",
        "1. Installs the necessary library (`langchain-pinecone`) for using Pinecone with LangChain.\n",
        "2. Imports the Pinecone client to manage vector data.\n",
        "3. Securely retrieves the Pinecone API key using Google Colab's `userdata` module.\n",
        "4. Sets the API key as an environment variable for secure usage.\n",
        "5. Initializes a connection to Pinecone, enabling further interactions with the vector database.\n"
      ],
      "metadata": {
        "id": "npukmQBFWA9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìÇ Import the ServerlessSpec class from the Pinecone library\n",
        "# This is used to specify serverless configuration for the index.\n",
        "from pinecone import ServerlessSpec\n",
        "\n",
        "# üìõ Define the name of the index\n",
        "# This is a unique identifier for your Pinecone index.\n",
        "index_name = \"rag-project\"\n",
        "\n",
        "# üèóÔ∏è Create a new Pinecone index\n",
        "# The index will store vector embeddings for fast similarity search.\n",
        "pc.create_index(\n",
        "    name=index_name,  # üîñ Name of the index\n",
        "    dimension=768,    # üìè Dimension of the vectors (e.g., 768 for BERT-like models)\n",
        "    metric=\"cosine\",  # üìê Metric used for similarity search (cosine similarity)\n",
        "    spec=ServerlessSpec(  # üõ†Ô∏è Configuration for the index's serverless setup\n",
        "        cloud=\"aws\",       # ‚òÅÔ∏è Cloud provider (AWS in this case)\n",
        "        region=\"us-east-1\" # üåç Region where the index is hosted\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "E55_nN-13HUb"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. `from pinecone import ServerlessSpec`**\n",
        "- **What it does**:\n",
        "  - Imports the `ServerlessSpec` class from the Pinecone library.\n",
        "  - `ServerlessSpec` is used to define the serverless deployment configuration for the Pinecone index.\n",
        "- **Why it‚Äôs important**:\n",
        "  - It specifies the cloud provider (e.g., AWS, Google Cloud) and the region where the index will be hosted.\n",
        "\n",
        "‚òÅÔ∏è **Specifying the cloud configuration!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. `index_name = \"rag-project1\"`**\n",
        "- **What it does**:\n",
        "  - Assigns the name `\"rag-project1\"` to the variable `index_name`.\n",
        "  - This will be the unique identifier for your Pinecone index.\n",
        "- **Why it‚Äôs important**:\n",
        "  - You‚Äôll use this name to reference and interact with the index later (e.g., for inserting or querying data).\n",
        "\n",
        "üìõ **Naming your Pinecone index!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. `pc.create_index(...)`**\n",
        "- **What it does**:\n",
        "  - Calls the `create_index` method on the Pinecone client (`pc`) to create a new index.\n",
        "  - The index is where vector embeddings will be stored for similarity search.\n",
        "\n",
        "---\n",
        "\n",
        "##### **3.1. `name=index_name`**\n",
        "- **What it does**:\n",
        "  - Specifies the name of the index being created.\n",
        "  - In this case, it‚Äôs `\"rag-project1\"`.\n",
        "- **Why it‚Äôs important**:\n",
        "  - Ensures the index is uniquely identifiable in your Pinecone workspace.\n",
        "\n",
        "üîñ **Index name: `rag-project1`!**\n",
        "\n",
        "---\n",
        "\n",
        "##### **3.2. `dimension=768`**\n",
        "- **What it does**:\n",
        "  - Specifies the dimensionality of the vectors to be stored in the index.\n",
        "  - For example, embeddings from BERT or OpenAI models often have a size of 768.\n",
        "- **Why it‚Äôs important**:\n",
        "  - The dimension must match the size of the vectors you‚Äôll insert into the index.\n",
        "\n",
        "üìè **Vector size: 768 dimensions!**\n",
        "\n",
        "---\n",
        "\n",
        "##### **3.3. `metric=\"cosine\"`**\n",
        "- **What it does**:\n",
        "  - Defines the similarity metric to use when comparing vectors.\n",
        "  - **Cosine similarity** measures how similar two vectors are, regardless of their magnitude.\n",
        "- **Other Options**:\n",
        "  - `\"euclidean\"`: Measures the straight-line distance between vectors.\n",
        "  - `\"dotproduct\"`: Measures the dot product of two vectors.\n",
        "- **Why it‚Äôs important**:\n",
        "  - The metric determines how similarity is calculated during retrieval.\n",
        "\n",
        "üìê **Using cosine similarity!**\n",
        "\n",
        "---\n",
        "\n",
        "##### **3.4. `spec=ServerlessSpec(...)`**\n",
        "- **What it does**:\n",
        "  - Specifies the serverless deployment configuration for the index.\n",
        "  - Pinecone automatically manages scaling and availability.\n",
        "  \n",
        "###### **3.4.1. `cloud=\"aws\"`**\n",
        "- **What it does**:\n",
        "  - Specifies the cloud provider where the index will be hosted (e.g., AWS, Google Cloud).\n",
        "- **Why it‚Äôs important**:\n",
        "  - Ensures the index is deployed on a cloud infrastructure you prefer.\n",
        "\n",
        "‚òÅÔ∏è **Cloud provider: AWS!**\n",
        "\n",
        "---\n",
        "\n",
        "###### **3.4.2. `region=\"us-east-1\"`**\n",
        "- **What it does**:\n",
        "  - Specifies the geographic region for hosting the index (e.g., `\"us-east-1\"` for the eastern United States).\n",
        "- **Why it‚Äôs important**:\n",
        "  - Choosing a region close to your application minimizes latency for read/write operations.\n",
        "\n",
        "üåç **Hosting region: US-East-1!**\n",
        "\n",
        "---\n",
        "\n",
        "### **Tabular Breakdown**\n",
        "\n",
        "| **Code Snippet**                           | **Explanation**                                                                 | **Highlights**                    |\n",
        "|--------------------------------------------|---------------------------------------------------------------------------------|--------------------------------|\n",
        "| `from pinecone import ServerlessSpec`      | Imports the `ServerlessSpec` class to define the serverless deployment setup.   | ‚òÅÔ∏è Specifying the cloud setup! |\n",
        "| `index_name = \"rag-project1\"`              | Assigns a unique name to your Pinecone index for future references.             | üìõ Naming the index!           |\n",
        "| `dimension=768`                            | Specifies the dimensionality of the vectors stored in the index.                | üìè Vector size: 768!           |\n",
        "| `metric=\"cosine\"`                          | Sets cosine similarity as the metric for vector comparisons.                   | üìê Cosine similarity!          |\n",
        "| `cloud=\"aws\"`                              | Deploys the index on Amazon Web Services (AWS).                                 | ‚òÅÔ∏è Cloud: AWS!                |\n",
        "| `region=\"us-east-1\"`                       | Hosts the index in the US East region for reduced latency.                      | üåç Region: US-East-1!          |\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "This code creates a **Pinecone index** with the following properties:\n",
        "1. **Name**: `rag-project1`.\n",
        "2. **Dimension**: 768 (matching the size of your embeddings).\n",
        "3. **Similarity Metric**: Cosine similarity.\n",
        "4. **Serverless Configuration**:\n",
        "   - Cloud Provider: AWS.\n",
        "   - Region: US-East-1.\n",
        "\n",
        "The index will store and retrieve vector embeddings efficiently, serving as a backbone for your **retrieval-augmented generation (RAG)** workflows.\n",
        "\n"
      ],
      "metadata": {
        "id": "4zy0nF_3XMdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# - google-generativeai: Provides access to Google's generative AI tools.\n",
        "# - pypdf: A modern library for handling PDFs.\n",
        "# - PyPDF2: An older but widely-used library for reading and writing PDFs.\n",
        "# --quiet: Suppresses unnecessary output during installation.\n",
        "!pip install google-generativeai pypdf PyPDF2 --quiet"
      ],
      "metadata": {
        "id": "MzsufYVe_44d"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. `google-generativeai`**\n",
        "- **What it does**:\n",
        "  - Installs the **Google Generative AI Python library**.\n",
        "  - This library lets you interact with Google's Generative AI models, such as Gemini, to generate text, embeddings, or other AI-driven outputs.\n",
        "- **Why it‚Äôs used**:\n",
        "  - Provides APIs for accessing Google Generative AI services.\n",
        "\n",
        "ü§ñ **Unlocking AI power with Google!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. `pypdf`**\n",
        "- **What it does**:\n",
        "  - Installs **PyPDF**, a modern and efficient library for handling PDFs.\n",
        "  - It supports features like reading, splitting, merging, and extracting text from PDF files.\n",
        "- **Why it‚Äôs used**:\n",
        "  - Simplifies working with PDF documents in Python.\n",
        "\n",
        "üìÑ **Handling PDFs with ease!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. `PyPDF2`**\n",
        "- **What it does**:\n",
        "  - Installs **PyPDF2**, an older library for PDF manipulation.\n",
        "  - Supports splitting, merging, encrypting, and text extraction from PDF files.\n",
        "- **Why it‚Äôs used**:\n",
        "  - Still widely used in many projects and provides compatibility for older workflows.\n",
        "\n",
        " üóÇÔ∏è **Legacy PDF support!**\n",
        "\n",
        "---\n",
        "\n",
        "### **Tabular Breakdown**\n",
        "\n",
        "| **Code Snippet**         | **Explanation**                                                   | **Highlights**                  |\n",
        "|---------------------------|-------------------------------------------------------------------|------------------------------|\n",
        "| `google-generativeai`    | Installs the library for accessing Google Generative AI models.  | ü§ñ Unlocking AI tools!       |\n",
        "| `pypdf`                  | Modern library for handling PDF documents.                       | üìÑ Modern PDF handling!      |\n",
        "| `PyPDF2`                 | Older library for legacy PDF handling and manipulation.          | üóÇÔ∏è Legacy PDF support!       |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "s1JEguRMYOZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üéâ Install LangChain Community tools\n",
        "!pip install -qU langchain-community\n",
        "\n",
        "# üìÇ Import PyPDFLoader from LangChain Community document loaders\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# üî† Import CharacterTextSplitter for splitting text into smaller chunks\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# üìÑ Load the PDF using PyPDFLoader\n",
        "# PyPDFLoader reads and processes PDF files.\n",
        "loader = PyPDFLoader(\"/content/Pak301 Handouts.pdf\")\n",
        "\n",
        "# ‚úÇÔ∏è Load and split the document\n",
        "# The `load_and_split` method returns a list of Document objects\n",
        "# Each Document contains the text and metadata from the PDF\n",
        "documents = loader.load_and_split()\n"
      ],
      "metadata": {
        "id": "gTPBDYW57rib"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tabular Breakdown**\n",
        "\n",
        "| **Code Snippet**                                      | **Explanation**                                                                                  | **Highlights**                   |\n",
        "|-------------------------------------------------------|--------------------------------------------------------------------------------------------------|-------------------------------|\n",
        "| `from langchain_community.document_loaders import PyPDFLoader` | Imports a class for processing PDF files and extracting their content.                          | üìÇ Loading PDFs with LangChain! |\n",
        "| `from langchain.text_splitter import CharacterTextSplitter`    | Imports a utility for splitting large text into smaller, manageable chunks.                     | üî† Chunking text efficiently! |\n",
        "| `loader = PyPDFLoader(\"/content/Pak301 Handouts.pdf\")`          | Initializes the loader to process the specified PDF file.                                        | üìÑ Loading Pak301 Handouts!   |\n",
        "| `documents = loader.load_and_split()`                           | Extracts content and splits it into multiple `Document` objects for further processing.          | ‚úÇÔ∏è Splitting text into chunks!|\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TcRha3OIZ4ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üõ†Ô∏è Initialize the text splitter\n",
        "# This splits the text into chunks of 500 characters with 100-character overlap between consecutive chunks.\n",
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "\n",
        "# üìÑ Extract the text content from Document objects\n",
        "# The `page_content` attribute contains the main text content of each document.\n",
        "texts = [doc.page_content for doc in documents]\n",
        "\n",
        "# ‚úÇÔ∏è Split the text into smaller chunks\n",
        "# The `create_documents` method processes the text and creates smaller, manageable chunks.\n",
        "docs = text_splitter.create_documents(texts)\n",
        "\n",
        "# üìä Display the number of chunks created\n",
        "# Prints the total number of chunks for verification.\n",
        "print(f\"Number of chunks: {len(docs)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhK_k2x3BdYd",
        "outputId": "aeef8603-d528-482c-ddec-3b3c5add71b0"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **1. `text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)`**\n",
        "- **What it does**:\n",
        "  - Creates a `CharacterTextSplitter` object that divides large texts into smaller chunks for easier processing.\n",
        "- **Parameters**:\n",
        "  - `chunk_size=500`: The maximum size of each chunk (500 characters in this case).\n",
        "  - `chunk_overlap=100`: Ensures consecutive chunks overlap by 100 characters, preserving context between chunks.\n",
        "- **Why it‚Äôs used**:\n",
        "  - Splitting text helps in processing and embedding large documents efficiently.\n",
        "\n",
        "‚úÇÔ∏è **Splitting text into chunks!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. `texts = [doc.page_content for doc in documents]`**\n",
        "- **What it does**:\n",
        "  - Extracts the main text (`page_content`) from each document in the `documents` list and stores it in the `texts` list.\n",
        "- **Why it‚Äôs used**:\n",
        "  - Prepares raw text data for splitting.\n",
        "- **Example**:\n",
        "  - If `documents` contains:\n",
        "    ```plaintext\n",
        "    [Document(page_content=\"Page 1 text\"), Document(page_content=\"Page 2 text\")]\n",
        "    ```\n",
        "    Then `texts` will be:\n",
        "    ```plaintext\n",
        "    [\"Page 1 text\", \"Page 2 text\"]\n",
        "    ```\n",
        "\n",
        "üìÑ **Extracting text from documents!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. `docs = text_splitter.create_documents(texts)`**\n",
        "- **What it does**:\n",
        "  - Splits the `texts` into smaller chunks based on the splitter configuration (`chunk_size` and `chunk_overlap`).\n",
        "- **Output**:\n",
        "  - A list of smaller documents, where each chunk preserves part of the original text.\n",
        "- **Why it‚Äôs used**:\n",
        "  - Chunks are more manageable for embedding models and ensure smooth retrieval in RAG workflows.\n",
        "\n",
        "üîç **Creating manageable text chunks!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. `print(f\"Number of chunks: {len(docs)}\")`**\n",
        "- **What it does**:\n",
        "  - Prints the total number of chunks generated from the splitting process.\n",
        "- **Why it‚Äôs used**:\n",
        "  - Verifies that the splitting worked as expected and gives an idea of the document's complexity.\n",
        "\n",
        "üìä **Counting text chunks!**\n",
        "\n",
        "---\n",
        "\n",
        "### **Tabular Breakdown**\n",
        "\n",
        "| **Code Snippet**                                 | **Explanation**                                                                 | **Highlights**                   |\n",
        "|--------------------------------------------------|---------------------------------------------------------------------------------|-------------------------------|\n",
        "| `CharacterTextSplitter(chunk_size=500, ...)`    | Splits text into smaller chunks of 500 characters with 100-character overlap.   | ‚úÇÔ∏è Splitting text!            |\n",
        "| `[doc.page_content for doc in documents]`       | Extracts the main text from the list of document objects.                       | üìÑ Extracting text!           |\n",
        "| `text_splitter.create_documents(texts)`         | Divides the text into chunks based on the splitter configuration.               | üîç Creating chunks!           |\n",
        "| `print(f\"Number of chunks: {len(docs)}\")`       | Displays the number of chunks created for verification.                         | üìä Counting chunks!           |\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "This code:\n",
        "1. Extracts text from a list of `Document` objects.\n",
        "2. Splits the text into smaller, overlapping chunks using `CharacterTextSplitter`.\n",
        "3. Prints the number of resulting chunks for validation.\n",
        "\n",
        "This is commonly used in workflows like **Retrieval-Augmented Generation (RAG)** to process and store documents for efficient retrieval.\n"
      ],
      "metadata": {
        "id": "_fq0kj7Aa8q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üéâ Import Google Generative AI Embeddings\n",
        "# This provides the embedding model for converting text into numerical vector representations.\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "# üìä Initialize the embedding model\n",
        "# Uses the \"models/embedding-001\" to create vector embeddings for text.\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "# üóÇÔ∏è Import Pinecone vector store\n",
        "# This allows storing and retrieving embeddings efficiently using Pinecone.\n",
        "from langchain_community.vectorstores import Pinecone\n",
        "\n",
        "# üèóÔ∏è Create a vector store from documents\n",
        "# Converts the text chunks into embeddings and stores them in a Pinecone index.\n",
        "vector_store = Pinecone.from_documents(\n",
        "    docs,                # üìù The text chunks to be converted into embeddings\n",
        "    embedding=embeddings,  # üìä The embedding model used for vectorization\n",
        "    index_name=index_name  # üìõ The name of the Pinecone index to store embeddings\n",
        ")  # Pass the index name to from_documents"
      ],
      "metadata": {
        "id": "R6JKG0Yr52bj"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. `from langchain_google_genai import GoogleGenerativeAIEmbeddings`**\n",
        "- **What it does**:\n",
        "  - Imports the `GoogleGenerativeAIEmbeddings` class from LangChain's Google Generative AI integration.\n",
        "- **Why it‚Äôs used**:\n",
        "  - To generate **vector embeddings** for text. These embeddings are numerical representations of text used for similarity searches.\n",
        "\n",
        "ü§ñ **AI-powered embeddings!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. `embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")`**\n",
        "- **What it does**:\n",
        "  - Initializes the embedding model from Google Generative AI.\n",
        "  - The model converts text into vectors (high-dimensional numerical representations).\n",
        "- **Parameters**:\n",
        "  - **`model=\"models/embedding-001\"`**: Specifies the embedding model to use (version 001 in this case).\n",
        "- **Why it‚Äôs used**:\n",
        "  - To prepare text chunks for insertion into the vector store.\n",
        "\n",
        "üìä **Embedding text into vectors!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. `from langchain_community.vectorstores import Pinecone`**\n",
        "- **What it does**:\n",
        "  - Imports the `Pinecone` vector store class from LangChain's community integrations.\n",
        "- **Why it‚Äôs used**:\n",
        "  - Pinecone stores and retrieves vector embeddings efficiently, enabling similarity searches for RAG workflows.\n",
        "\n",
        "üóÇÔ∏è **Efficient vector storage!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. `vector_store = Pinecone.from_documents(...)`**\n",
        "- **What it does**:\n",
        "  - Creates a vector store by:\n",
        "    1. Converting `docs` into embeddings using the specified `embeddings` model.\n",
        "    2. Storing these embeddings in the Pinecone index specified by `index_name`.\n",
        "- **Parameters**:\n",
        "  - **`docs`**: A list of text chunks to be embedded and stored.\n",
        "  - **`embedding=embeddings`**: The embedding model used to convert text into vectors.\n",
        "  - **`index_name=index_name`**: Specifies the Pinecone index where the embeddings will be stored.\n",
        "- **Why it‚Äôs used**:\n",
        "  - To prepare a searchable database of embeddings for text chunks, enabling fast similarity-based retrieval.\n",
        "\n",
        "üöÄ **Building the vector database!**\n",
        "\n",
        "---\n",
        "\n",
        "### **Tabular Breakdown**\n",
        "\n",
        "| **Code Snippet**                                 | **Explanation**                                                                 | **Highlights**                     |\n",
        "|--------------------------------------------------|---------------------------------------------------------------------------------|---------------------------------|\n",
        "| `from langchain_google_genai import ...`        | Imports the embedding model class from LangChain‚Äôs Google integration.          | ü§ñ AI-powered embeddings!       |\n",
        "| `GoogleGenerativeAIEmbeddings(model=...)`       | Initializes the embedding model to generate vector representations of text.     | üìä Embedding text into vectors! |\n",
        "| `from langchain_community.vectorstores import Pinecone` | Imports Pinecone for storing and retrieving embeddings.                         | üóÇÔ∏è Efficient vector storage!    |\n",
        "| `Pinecone.from_documents(...)`                  | Converts text chunks into embeddings and stores them in the Pinecone index.     | üöÄ Building the vector database!|\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "This code:\n",
        "1. **Generates embeddings** for text chunks using Google's embedding model.\n",
        "2. **Stores these embeddings** in a Pinecone vector database for fast similarity searches.\n",
        "3. Prepares the foundation for a **Retrieval-Augmented Generation (RAG)** workflow.\n"
      ],
      "metadata": {
        "id": "3R01VeQNbVDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÇÔ∏è Define a function to format documents\n",
        "# This combines the text content of multiple documents into a single string, separated by double newlines.\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# üîç Convert the vector store into a retriever\n",
        "# The retriever fetches the top 3 most similar documents for a given query.\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# üß™ Print the type of the retriever object\n",
        "# This helps verify the type of the retriever created (e.g., PineconeRetriever).\n",
        "print(type(retriever))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnuFOi4EPoAk",
        "outputId": "1f59683e-98b4-4dd8-8b50-fc9ffca71703"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. `def format_docs(docs):`\n",
        "- **What it does**:\n",
        "  - Defines a function called `format_docs` that takes a list of documents (`docs`) as input.\n",
        "- **Why it‚Äôs used**:\n",
        "  - Prepares document content in a human-readable format by combining multiple text chunks.\n",
        "\n",
        "üìù **Combining text chunks!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. `return \"\\n\\n\".join(doc.page_content for doc in docs)`**\n",
        "- **What it does**:\n",
        "  - Combines the `page_content` of each document in the `docs` list into a single string.\n",
        "  - Separates each document's content with two newline characters (`\\n\\n`) for better readability.\n",
        "- **Why it‚Äôs used**:\n",
        "  - Prepares the documents for display or as context for an LLM.\n",
        "- **Example**:\n",
        "  - Input: `docs = [Document(page_content=\"Text1\"), Document(page_content=\"Text2\")]`\n",
        "  - Output:\n",
        "    ```plaintext\n",
        "    Text1\n",
        "\n",
        "    Text2\n",
        "    ```\n",
        "\n",
        "üîó **Formatting document text!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. `retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})`**\n",
        "- **What it does**:\n",
        "  - Converts the `vector_store` into a **retriever** object using the `.as_retriever` method.\n",
        "  - Retrieves the top `k=3` most similar documents for any given query.\n",
        "- **Parameter**:\n",
        "  - **`search_kwargs={\"k\": 3}`**: Specifies that the retriever should return the **top 3 results**.\n",
        "- **Why it‚Äôs used**:\n",
        "  - A retriever enables similarity searches in the vector store, forming the backbone of RAG workflows.\n",
        "\n",
        "üîç **Fetching top 3 matches!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. `print(type(retriever))`**\n",
        "- **What it does**:\n",
        "  - Prints the type of the `retriever` object to confirm the retriever has been properly created.\n",
        "- **Why it‚Äôs used**:\n",
        "  - Debugging step to verify that `retriever` is an instance of the expected class (e.g., `PineconeRetriever`).\n",
        "\n",
        "üß™ **Testing the retriever's type!**\n",
        "\n",
        "---\n",
        "\n",
        "### **Tabular Breakdown**\n",
        "\n",
        "| **Code Snippet**                             | **Explanation**                                                       | **Highlights**                     |\n",
        "|----------------------------------------------|------------------------------------------------------------------------|---------------------------------|\n",
        "| `def format_docs(docs):`                     | Defines a function to format document content into a readable string. | üìù Combining text chunks!       |\n",
        "| `\"\\n\\n\".join(doc.page_content for doc in docs)` | Combines multiple documents' content with double newlines for readability. | üîó Formatting document text!    |\n",
        "| `vector_store.as_retriever(...)`             | Converts the vector store into a retriever for similarity search.      | üîç Fetching top 3 matches!      |\n",
        "| `print(type(retriever))`                     | Prints the retriever‚Äôs type for verification during debugging.          | üß™ Testing the retriever's type!|\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "This code:\n",
        "1. Defines a `format_docs` function to prepare text from documents for readability.\n",
        "2. Converts a vector store into a retriever, fetching the top 3 similar documents.\n",
        "3. Prints the type of the retriever object for debugging purposes.\n"
      ],
      "metadata": {
        "id": "piAeGb_6cJad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ü§ñ Import the ChatGoogleGenerativeAI class\n",
        "# This allows interaction with Google's Gemini language models through LangChain.\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# üß† Initialize the LLM (Large Language Model)\n",
        "# Creates an instance of the \"gemini-1.5-flash\" model for conversational AI tasks.\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "NlD_1FdJJXLE"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. `from langchain_google_genai import ChatGoogleGenerativeAI`**\n",
        "- **What it does**:\n",
        "  - Imports the `ChatGoogleGenerativeAI` class, which is part of LangChain's integration with Google's Generative AI models.\n",
        "- **Why it‚Äôs used**:\n",
        "  - Enables you to interact with Google's **Gemini models** for tasks like text generation, summarization, question answering, and more.\n",
        "\n",
        "üöÄ **Bringing Google's AI to your code!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. `llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")`**\n",
        "- **What it does**:\n",
        "  - Initializes an instance of the Gemini model for conversational AI.\n",
        "  - The `model` parameter specifies the version and type of the Gemini model being used.\n",
        "- **Parameters**:\n",
        "  - **`model=\"gemini-1.5-flash\"`**:\n",
        "    - Refers to **Gemini 1.5 Flash**, an optimized version of the Gemini model designed for fast and lightweight operations.\n",
        "- **Why it‚Äôs used**:\n",
        "  - The `llm` object can now be used to send prompts and receive responses from the Gemini LLM.\n",
        "\n",
        "üß† **Powering conversational AI!**\n",
        "\n",
        "---\n",
        "\n",
        "### **How It Works in Context**\n",
        "\n",
        "Once initialized, you can use the `llm` object to interact with the Gemini model by passing prompts or queries. For example:\n",
        "\n",
        "```python\n",
        "response = llm.invoke(\"What is the capital of France?\")\n",
        "print(response)\n",
        "```\n",
        "\n",
        "This will send the query to the Gemini model and return the answer.\n",
        "\n",
        "---\n",
        "\n",
        "### **Tabular Breakdown**\n",
        "\n",
        "| **Code Snippet**                              | **Explanation**                                                       | **Highlights**                      |\n",
        "|-----------------------------------------------|-----------------------------------------------------------------------|----------------------------------|\n",
        "| `from langchain_google_genai import ...`     | Imports the integration for using Google's Generative AI models.       | üöÄ Bringing AI to your code!     |\n",
        "| `ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")` | Creates an instance of the Gemini 1.5 Flash model for conversational AI. | üß† Powering conversational AI!   |\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "This code:\n",
        "1. Imports the necessary tools to interact with Google's Generative AI models via LangChain.\n",
        "2. Initializes the Gemini 1.5 Flash model, which is optimized for fast and lightweight conversational tasks."
      ],
      "metadata": {
        "id": "vCh8HOhqcfG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üì© Import SystemMessage\n",
        "# Used to define system-level instructions for the AI's behavior.\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "# üóÇÔ∏è Import ChatPromptTemplate and HumanMessagePromptTemplate\n",
        "# These classes are used to structure the AI's conversational prompts.\n",
        "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# üé® Create a Chat Prompt Template\n",
        "# Combines system and human message templates into a structured prompt for the AI.\n",
        "chat_template = ChatPromptTemplate.from_messages([\n",
        "    # üõ†Ô∏è System Message Template\n",
        "    # Defines the AI's role and overarching behavior as an expert PAKSTUDY scholar.\n",
        "    SystemMessage(content=\"\"\"You are an expert level PAKSTUDY scholar.\n",
        "                  Given a context and question from user,\n",
        "                  you should answer based on the given context.\"\"\"),\n",
        "\n",
        "    # üßë Human Message Prompt Template\n",
        "    # Specifies the format of the user's query and AI's expected response.\n",
        "    HumanMessagePromptTemplate.from_template(\"\"\"Answer the question based on the given context.\n",
        "    Context: {context}\n",
        "    Question: {question}\n",
        "    Answer: \"\"\")\n",
        "])"
      ],
      "metadata": {
        "id": "C59Jbt3wPKK0"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. `from langchain_core.messages import SystemMessage`**\n",
        "- **What it does**:\n",
        "  - Imports the `SystemMessage` class from LangChain's core module.\n",
        "- **Why it‚Äôs used**:\n",
        "  - To define **system-level instructions** that establish the AI's behavior and purpose.\n",
        "- **Example**:\n",
        "  - \"You are an expert in PAKSTUDY\" is a system instruction ensuring the AI maintains a scholar-like tone.\n",
        "\n",
        "üõ†Ô∏è **Setting AI‚Äôs role and behavior!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. `from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate`**\n",
        "- **What it does**:\n",
        "  - Imports classes to structure the prompts that guide the AI's interactions.\n",
        "  - **`ChatPromptTemplate`**: Combines multiple message templates into one cohesive prompt.\n",
        "  - **`HumanMessagePromptTemplate`**: Specifies how the user's query (human input) is structured.\n",
        "- **Why it‚Äôs used**:\n",
        "  - Ensures the AI understands and responds to queries in a structured and consistent way.\n",
        "\n",
        "üìã **Organizing user-AI communication!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. `SystemMessage(content=...)`**\n",
        "- **What it does**:\n",
        "  - Creates a message that defines the AI's **role and behavior**.\n",
        "- **Content**:\n",
        "  - \"You are an expert level PAKSTUDY scholar...\":\n",
        "    - Establishes the AI's identity and expertise.\n",
        "  - \"Given a context and question from user...\":\n",
        "    - Ensures the AI only answers based on the provided context, reducing hallucination.\n",
        "- **Why it‚Äôs used**:\n",
        "  - Guides the AI to behave as a knowledgeable, focused scholar in the domain of **PAKSTUDY**.\n",
        "\n",
        "üéì **Making AI an expert PAKSTUDY scholar!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. `HumanMessagePromptTemplate.from_template(...)`**\n",
        "- **What it does**:\n",
        "  - Defines the **format** of the user‚Äôs input and the AI‚Äôs response.\n",
        "- **Template**:\n",
        "  ```plaintext\n",
        "  Answer the question based on the given context.\n",
        "  Context: {context}\n",
        "  Question: {question}\n",
        "  Answer:\n",
        "  ```\n",
        "  - **`{context}`**: Placeholder for the user-provided context.\n",
        "  - **`{question}`**: Placeholder for the user‚Äôs question.\n",
        "  - **`Answer:`**: Prompts the AI to generate a response.\n",
        "- **Why it‚Äôs used**:\n",
        "  - Ensures the AI focuses on the context to answer the user‚Äôs query accurately.\n",
        "\n",
        "üßë **Formatting human-AI interaction!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. `ChatPromptTemplate.from_messages([...])`**\n",
        "- **What it does**:\n",
        "  - Combines the system and human message templates into a single structured prompt.\n",
        "- **Why it‚Äôs used**:\n",
        "  - Provides the AI with both role-defining instructions and query-specific input in a unified format.\n",
        "\n",
        "üé® **Combining templates for AI prompts!**\n",
        "\n",
        "---\n",
        "\n",
        "### **Tabular Breakdown**\n",
        "\n",
        "| **Code Snippet**                                | **Explanation**                                                                 | **Highlights**                     |\n",
        "|-------------------------------------------------|---------------------------------------------------------------------------------|---------------------------------|\n",
        "| `SystemMessage(content=...)`                   | Sets the AI‚Äôs role and behavior as a PAKSTUDY scholar, ensuring domain expertise. | üéì AI as a PAKSTUDY scholar!    |\n",
        "| `HumanMessagePromptTemplate.from_template(...)` | Defines the structure for the user‚Äôs input and AI‚Äôs response format.             | üßë Structuring input/output!    |\n",
        "| `ChatPromptTemplate.from_messages([...])`      | Combines system and human templates into a single structured prompt.             | üé® Combining templates!         |\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "This code:\n",
        "1. **Defines the AI‚Äôs role** as a **PAKSTUDY scholar** using a `SystemMessage`.\n",
        "2. **Structures user interactions** with a `HumanMessagePromptTemplate`.\n",
        "3. **Unifies everything** with a `ChatPromptTemplate`, guiding the AI to provide accurate, context-based responses.\n"
      ],
      "metadata": {
        "id": "b9dTSZLPc-N1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üì§ Import RunnablePassthrough\n",
        "# Passes data through the chain without modifying it. Used for the \"question\" input.\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# üìù Import StrOutputParser\n",
        "# Converts the output of the chain into a simple string format.\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# üé® Initialize the output parser\n",
        "# Prepares the raw output from the chain to be returned as a clean string.\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# üèóÔ∏è Build the Retrieval-Augmented Generation (RAG) chain\n",
        "# Combines context retrieval, prompt construction, LLM interaction, and output parsing.\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}  # Inputs\n",
        "    | chat_template  # Combines context and question into a structured prompt\n",
        "    | llm            # Sends the prompt to the LLM for generating a response\n",
        "    | output_parser  # Converts the LLM's output into a clean string\n",
        ")"
      ],
      "metadata": {
        "id": "Ni_jtslYP-Ml"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. `from langchain_core.runnables import RunnablePassthrough`**\n",
        "- **What it does**:\n",
        "  - Imports the `RunnablePassthrough` class, which passes input data (like the question) through the chain without modifying it.\n",
        "- **Why it‚Äôs used**:\n",
        "  - The question input doesn‚Äôt require processing before it‚Äôs used, so this keeps it unchanged.\n",
        "\n",
        "üõ§Ô∏è **Passing the question as is!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. `from langchain_core.output_parsers import StrOutputParser`**\n",
        "- **What it does**:\n",
        "  - Imports the `StrOutputParser` class, which formats the output of the chain as a plain string.\n",
        "- **Why it‚Äôs used**:\n",
        "  - Simplifies the LLM's raw response, making it easier to handle and display.\n",
        "\n",
        "‚ú® **Formatting the output into plain text!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. `output_parser = StrOutputParser()`**\n",
        "- **What it does**:\n",
        "  - Creates an instance of `StrOutputParser` to process and clean the output.\n",
        "- **Why it‚Äôs used**:\n",
        "  - Ensures that the chain‚Äôs output is returned in a consistent string format.\n",
        "\n",
        "üìù **Initializing the output parser!**\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. `rag_chain = {... | ...}`**\n",
        "- **What it does**:\n",
        "  - Defines a **Retrieval-Augmented Generation (RAG) chain** that integrates retrieval, prompt creation, LLM processing, and output parsing.\n",
        "- **Components**:\n",
        "  - **`{\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}`**:\n",
        "    - **`retriever`**: Fetches relevant documents based on the query.\n",
        "    - **`format_docs`**: Combines retrieved documents into a single formatted string.\n",
        "    - **`RunnablePassthrough()`**: Passes the user‚Äôs question unchanged to the next step.\n",
        "  - **`| chat_template`**:\n",
        "    - Combines the retrieved context and question into a structured prompt for the LLM.\n",
        "  - **`| llm`**:\n",
        "    - Sends the prompt to the LLM (e.g., Google Gemini) for generating a response.\n",
        "  - **`| output_parser`**:\n",
        "    - Converts the raw response from the LLM into a clean, user-friendly string.\n",
        "\n",
        "üîó **Building the RAG workflow chain!**\n",
        "\n",
        "---\n",
        "\n",
        "### **Tabular Breakdown**\n",
        "\n",
        "| **Code Snippet**                              | **Explanation**                                                       | **Highlights**                     |\n",
        "|-----------------------------------------------|-----------------------------------------------------------------------|---------------------------------|\n",
        "| `RunnablePassthrough()`                       | Passes the question input unchanged through the chain.                | üõ§Ô∏è Passing input unchanged!     |\n",
        "| `StrOutputParser()`                           | Formats the LLM‚Äôs output into a clean string format.                  | ‚ú® Formatting output text!       |\n",
        "| `{\"context\": retriever | format_docs, ...}`   | Retrieves relevant documents, formats them, and passes the question.  | üîç Retrieving context!          |\n",
        "| `| chat_template`                             | Structures the prompt using the retrieved context and question.       | üìù Structuring the prompt!      |\n",
        "| `| llm`                                       | Sends the prompt to the LLM for generating a response.                | ü§ñ Generating answers!          |\n",
        "| `| output_parser`                             | Converts the raw output into a clean, readable string.                | üßπ Cleaning up the output!      |\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "This code sets up a **RAG (Retrieval-Augmented Generation) chain**:\n",
        "1. **Inputs**:\n",
        "   - Retrieves relevant context (`retriever | format_docs`).\n",
        "   - Passes the user question (`RunnablePassthrough`).\n",
        "2. **Processing**:\n",
        "   - Combines inputs into a structured prompt using `chat_template`.\n",
        "   - Sends the prompt to the LLM for response generation.\n",
        "3. **Output**:\n",
        "   - Parses the raw LLM output into a clean string using `StrOutputParser`."
      ],
      "metadata": {
        "id": "tAQ2zgNndg9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üõ†Ô∏è Invoke the RAG chain\n",
        "# Sends the query to the RAG chain\n",
        "response = rag_chain.invoke(\"ROLE OF SIR SYED IN EDUCATION OF MUSLIMS OF SUBCONTINENT\")\n",
        "\n",
        "# üì§ Import Markdown and display from IPython\n",
        "# Used to format and display the response in a more readable Markdown format.\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# üñ•Ô∏è Display the response\n",
        "# Formats the response as Markdown and displays it.\n",
        "display(Markdown(response))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "prtnkwopQQhc",
        "outputId": "7ff882b7-c5d7-438a-d85c-5db6a8a52fef"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sir Syed Ahmad Khan played a pivotal role in reforming Muslim education in the subcontinent.  He saw the need for Muslims to adopt modern Western education to improve their standing in society and counter the backwardness and humiliation they faced after the 1857 War of Independence.  He established educational institutions and dedicated his life to bringing Muslims closer to the British, believing that Western learning was key to their future prosperity.  His efforts focused on bridging the gap between Muslims and Western knowledge, thereby empowering his community.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Code Snippet**                                                | **Explanation**                                                                                          | **Highlights**                     |\n",
        "|------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|---------------------------------|\n",
        "| `response = rag_chain.invoke(...)`                              | Sends the query to the RAG chain to retrieve context and generate a response using the LLM.              | üõ†Ô∏è Sending the query!           |\n",
        "| `from IPython.display import Markdown, display`                 | Imports tools to format and display the response as Markdown.                                            | üì§ Formatting tools!            |\n",
        "| `display(Markdown(response))`                                   | Converts the response into a Markdown format and displays it.                                            | üñ•Ô∏è Readable output!             |\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "This code:\n",
        "1. **Queries the RAG chain**: Sends a question to retrieve and generate a relevant response.\n",
        "2. **Formats the response**: Converts the response into Markdown for better readability.\n",
        "3. **Displays the result**: Shows the processed response in the notebook interface."
      ],
      "metadata": {
        "id": "mbz3BjcOePBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîç Use the retriever to fetch context\n",
        "# Sends the query \"ROLE OF SIR SYED IN EDUCATION OF MUSLIMS OF SUBCONTINENT\" to the retriever\n",
        "# Retrieves the most relevant documents based on the query.\n",
        "response = retriever.invoke(\"ROLE OF SIR SYED IN EDUCATION OF MUSLIMS OF SUBCONTINENT\")\n",
        "\n",
        "# üìã Display the raw response\n",
        "# Prints the response from the retriever, which contains relevant documents or chunks.\n",
        "response\n"
      ],
      "metadata": {
        "id": "NK2BtJeYRX5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Code Snippet**                                                | **Explanation**                                                                                          | **Highlights**                     |\n",
        "|------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|---------------------------------|\n",
        "| `response = retriever.invoke(...)`                              | Sends the query to the retriever, which fetches relevant documents or text chunks.                      | üîç Fetching context!            |\n",
        "| `response`                                                      | Displays the raw response from the retriever for review or further processing.                          | üìã Raw response!                |\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "This code:\n",
        "1. **Queries the retriever**: Retrieves relevant documents or chunks related to the query.\n",
        "2. **Displays the results**: Shows the fetched context, which can be passed to the next stage for LLM processing."
      ],
      "metadata": {
        "id": "zX_uMczuej71"
      }
    }
  ]
}